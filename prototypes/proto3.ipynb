{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "5d591f94785818b435df4881258bbd57da528693019fb2c63deaaf29b9986dd3"
    },
    "colab": {
      "name": "proto3.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prototype 3"
      ],
      "metadata": {
        "id": "cCZn1_1vBrkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font size=\"3\"> Prototype 3 introduces fixes that seek to solve the problem that a two-player environment brings to the deep-Q-learning algorithm. This requires a revamping of the indexing system. Every record must be seperated by which side's move has produced the result. This makes sure that a checkmate from white does not back-propagate high Q values for black's mistakes that lead to the checkmate.</font>"
      ],
      "metadata": {
        "id": "Vn1-o9bzBrku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "gamma = 0.99  \r\n",
        "epsilon = 1  \r\n",
        "epsilon_min = 0.1  \r\n",
        "epsilon_max = 1.0  \r\n",
        "epsilon_interval = (\r\n",
        "    epsilon_max - epsilon_min\r\n",
        ")  \r\n",
        "batch_size = 32  \r\n",
        "max_steps_per_episode = 200\r\n",
        "num_actions = 4096"
      ],
      "outputs": [],
      "metadata": {
        "id": "caREoUMSBrkx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from IPython.display import clear_output\r\n",
        "from tensorflow import keras\r\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\r\n",
        "\r\n",
        "running_reward = 0\r\n",
        "episode_count = 0\r\n",
        "frame_count = 0\r\n",
        "\r\n",
        "epsilon_random_frames = 50000\r\n",
        "epsilon_greedy_frames = 1000000.0\r\n",
        "max_memory_length = 100000\r\n",
        "update_after_actions = 4\r\n",
        "update_target_network = 100\r\n",
        "loss_function = keras.losses.Huber()\r\n",
        "len_episodes = 0\r\n",
        "iterations = 1000"
      ],
      "outputs": [],
      "metadata": {
        "id": "rWoic6ehBrk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import chess\r\n",
        "def filter_legal_moves(board,logits):\r\n",
        "    filter_mask = np.zeros(logits.shape)\r\n",
        "    legal_moves = board.legal_moves\r\n",
        "    for legal_move in legal_moves:\r\n",
        "        from_square = legal_move.from_square\r\n",
        "        to_square = legal_move.to_square\r\n",
        "        idx = move2num[chess.Move(from_square,to_square)]\r\n",
        "        filter_mask[idx] = 1\r\n",
        "    new_logits = logits*filter_mask\r\n",
        "    return new_logits\r\n",
        "\r\n",
        "num2move = {}\r\n",
        "move2num = {}\r\n",
        "counter = 0\r\n",
        "for from_sq in range(64):\r\n",
        "    for to_sq in range(64):\r\n",
        "        num2move[counter] = chess.Move(from_sq,to_sq)\r\n",
        "        move2num[chess.Move(from_sq,to_sq)] = counter\r\n",
        "        counter += 1\r\n",
        "\r\n",
        "def translate_board(board): \r\n",
        "    pgn = board.epd()\r\n",
        "    foo = []  \r\n",
        "    pieces = pgn.split(\" \", 1)[0]\r\n",
        "    rows = pieces.split(\"/\")\r\n",
        "    for row in rows:\r\n",
        "        foo2 = []  \r\n",
        "        for thing in row:\r\n",
        "            if thing.isdigit():\r\n",
        "                for i in range(0, int(thing)):\r\n",
        "                    foo2.append(chess_dict['.'])\r\n",
        "            else:\r\n",
        "                foo2.append(chess_dict[thing])\r\n",
        "        foo.append(foo2)\r\n",
        "    return np.array(foo)\r\n",
        "\r\n",
        "chess_dict = {\r\n",
        "    'p' : [1,0,0,0,0,0,0,0,0,0,0,0],\r\n",
        "    'P' : [0,0,0,0,0,0,1,0,0,0,0,0],\r\n",
        "    'n' : [0,1,0,0,0,0,0,0,0,0,0,0],\r\n",
        "    'N' : [0,0,0,0,0,0,0,1,0,0,0,0],\r\n",
        "    'b' : [0,0,1,0,0,0,0,0,0,0,0,0],\r\n",
        "    'B' : [0,0,0,0,0,0,0,0,1,0,0,0],\r\n",
        "    'r' : [0,0,0,1,0,0,0,0,0,0,0,0],\r\n",
        "    'R' : [0,0,0,0,0,0,0,0,0,1,0,0],\r\n",
        "    'q' : [0,0,0,0,1,0,0,0,0,0,0,0],\r\n",
        "    'Q' : [0,0,0,0,0,0,0,0,0,0,1,0],\r\n",
        "    'k' : [0,0,0,0,0,1,0,0,0,0,0,0],\r\n",
        "    'K' : [0,0,0,0,0,0,0,0,0,0,0,1],\r\n",
        "    '.' : [0,0,0,0,0,0,0,0,0,0,0,0],\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "9F2sspofCMPe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "from keras.layers import Dense,Flatten,Reshape\r\n",
        "from keras.layers.convolutional import Conv2D\r\n",
        "from keras.models import Model, Input\r\n",
        "class Q_model():\r\n",
        "    def __init__(self):\r\n",
        "        self.model = self.create_q_model()\r\n",
        "\r\n",
        "    def create_q_model(self):\r\n",
        "    # Network defined by the Deepmind paper\r\n",
        "        input_layer = Input(shape=(8, 8, 12))\r\n",
        "\r\n",
        "        # Convolutions on the frames on the screen\r\n",
        "        x = Conv2D(filters=64,kernel_size = 2,strides = (2,2))(input_layer)\r\n",
        "        x = Conv2D(filters=128,kernel_size=2,strides = (2,2))(x)\r\n",
        "        x = Conv2D(filters=256,kernel_size=2,strides = (2,2))(x)\r\n",
        "        x = Flatten()(x)\r\n",
        "\r\n",
        "        action = Dense(4096,activation = 'softmax')(x)\r\n",
        "        return Model(inputs=input_layer, outputs=action)\r\n",
        "    \r\n",
        "    def predict(self,env):\r\n",
        "        state_tensor = tf.convert_to_tensor(env.translate_board())\r\n",
        "        state_tensor = tf.expand_dims(state_tensor, 0)\r\n",
        "        action_probs = self.model(state_tensor, training=False)\r\n",
        "        action_space = filter_legal_moves(env.board,action_probs[0])\r\n",
        "        action = np.argmax(action_space, axis=None)\r\n",
        "        move= num2move[action]\r\n",
        "        return move,action\r\n",
        "    \r\n",
        "    def explore(self,env):\r\n",
        "        action_space = np.random.randn(4096)\r\n",
        "        action_space = filter_legal_moves(env.board,action_space)\r\n",
        "        action = np.argmax(action_space, axis=None)\r\n",
        "        move= num2move[action]\r\n",
        "        return move,action\r\n",
        "        \r\n",
        "    \r\n",
        "model = Q_model()\r\n",
        "model_target = Q_model()"
      ],
      "outputs": [],
      "metadata": {
        "id": "jGWyKrnPBrlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "import chess\r\n",
        "class ChessEnv():\r\n",
        "    def __init__(self):\r\n",
        "        self.board = chess.Board()\r\n",
        "        self.action_history = {\r\n",
        "            'white' : [],\r\n",
        "            'black' : [],\r\n",
        "        }\r\n",
        "        self.state_history = {\r\n",
        "            'white' : [],\r\n",
        "            'black' : [],\r\n",
        "        }\r\n",
        "        self.state_next_history = {\r\n",
        "            'white' : [],\r\n",
        "            'black' : [],\r\n",
        "        }\r\n",
        "        self.rewards_history = {\r\n",
        "            'white' : [],\r\n",
        "            'black' : [],\r\n",
        "        }\r\n",
        "        self.done_history = {\r\n",
        "            'white' : [],\r\n",
        "            'black' : [],\r\n",
        "        }\r\n",
        "        self.episode_reward_history = []\r\n",
        "        self.move_counter = 1\r\n",
        "        self.fast_counter = 0\r\n",
        "        self.pgn = ''\r\n",
        "        self.pgns = []\r\n",
        "        pass\r\n",
        "    def translate_board(self):\r\n",
        "        return translate_board(self.board)\r\n",
        "    \r\n",
        "    def reset(self):\r\n",
        "        self.board = chess.Board()\r\n",
        "        self.pgns.append(self.pgn)\r\n",
        "        self.move_counter = 1\r\n",
        "        self.fast_counter = 0\r\n",
        "        self.pgn = ''\r\n",
        "        for turn in self.rewards_history.keys():\r\n",
        "            if len(self.rewards_history[turn]) > max_memory_length:\r\n",
        "                del self.rewards_history[turn][:1]\r\n",
        "                del self.state_history[turn][:1]\r\n",
        "                del self.state_next_history[turn][:1]\r\n",
        "                del self.action_history[turn][:1]\r\n",
        "                del self.done_history[turn][:1]\r\n",
        "\r\n",
        "        if len(self.pgns) > 1000:\r\n",
        "          self.pgns.pop(-1)\r\n",
        "        return translate_board(self.board)\r\n",
        "\r\n",
        "    def update_pgn(self,move):\r\n",
        "      if self.fast_counter % 2 == 0:\r\n",
        "          self.pgn += str(self.move_counter)+ '.'\r\n",
        "          self.move_counter += 1\r\n",
        "      self.fast_counter += 1\r\n",
        "      string = str(self.board.san(move))+' '\r\n",
        "      self.pgn+=string\r\n",
        "      \r\n",
        "    \r\n",
        "    def step(self,action):\r\n",
        "        if self.board.turn:\r\n",
        "            turn = 'white'\r\n",
        "            opp = 'black'\r\n",
        "        else:\r\n",
        "            turn = 'black'\r\n",
        "            opp = 'white'\r\n",
        "        reward = 0\r\n",
        "        \r\n",
        "        state = self.translate_board()\r\n",
        "        self.update_pgn(action)\r\n",
        "        self.board.push(action)\r\n",
        "        \r\n",
        "        state_next = self.board\r\n",
        "        state_next = translate_board(state_next)\r\n",
        "        \r\n",
        "        if self.board.is_checkmate():\r\n",
        "            reward = 100\r\n",
        "\r\n",
        "        env.done = self.board.is_game_over()\r\n",
        "\r\n",
        "        self.action_history[turn].append(move2num[action])\r\n",
        "        self.state_history[turn].append(state)\r\n",
        "        self.state_next_history[turn].append(state_next)\r\n",
        "        self.done_history[turn].append(self.done)\r\n",
        "        self.rewards_history[turn].append(reward)\r\n",
        "        self.rewards_history[opp].append(-reward)\r\n",
        "        \r\n",
        "    def update_q_values(self):\r\n",
        "        sides = ['white','black']\r\n",
        "        state_samples = []\r\n",
        "        masks = []\r\n",
        "        updated_q_values = []\r\n",
        "        for turn in sides:\r\n",
        "            indices = np.random.choice(range(len(self.done_history[turn])), size=batch_size)\r\n",
        "            #Not only the iterations that have been complete. Using done_history to measure len is arbitrary\r\n",
        "                \r\n",
        "            state_sample = np.array([self.state_history[turn][i] for i in indices])\r\n",
        "            state_next_sample = np.array([self.state_next_history[turn][i] for i in indices])\r\n",
        "            rewards_sample = [self.rewards_history[turn][i] for i in indices]\r\n",
        "            action_sample = [self.action_history[turn][i] for i in indices]\r\n",
        "            done_sample = tf.convert_to_tensor(\r\n",
        "                [float(self.done_history[turn][i]) for i in indices]\r\n",
        "            )\r\n",
        "            \r\n",
        "            future_rewards = model_target.model.predict(state_next_sample)\r\n",
        "            \r\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\r\n",
        "                future_rewards, axis=1\r\n",
        "            )\r\n",
        "\r\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\r\n",
        "            masks = tf.one_hot(action_sample, num_actions)\r\n",
        "            \r\n",
        "            state_samples.append(state_sample)\r\n",
        "            masks.append(masks)\r\n",
        "            updated_q_values.append(updated_q_values)\r\n",
        "        return state_sample,masks,updated_q_values\r\n",
        "    \r\n",
        "env = ChessEnv()"
      ],
      "outputs": [],
      "metadata": {
        "id": "15LBsSSYBrk2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from tensorflow import keras\r\n",
        "for _ in range(iterations):\r\n",
        "    state = np.array(env.reset())\r\n",
        "    episode_reward = 0\r\n",
        "    len_episodes += 1\r\n",
        "    for timestep in range(1, max_steps_per_episode):\r\n",
        "        frame_count += 1\r\n",
        "\r\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\r\n",
        "            move,action = model.explore(env)\r\n",
        "        else:\r\n",
        "            move,action = model.predict(env)\r\n",
        "            \r\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\r\n",
        "        epsilon = max(epsilon, epsilon_min)\r\n",
        "        \r\n",
        "        env.step(move)\r\n",
        "\r\n",
        "        if frame_count % update_after_actions == 0 and len(env.done_history) > batch_size:\r\n",
        "            state_samples,masks,updated_q_values = env.update_q_values()\r\n",
        "            \r\n",
        "            for i in range(len(state_samples)):\r\n",
        "                with tf.GradientTape() as tape:\r\n",
        "                    q_values = model.model(state_samples[i])\r\n",
        "                    q_action = tf.reduce_sum(tf.multiply(q_values, masks[i]), axis=1)\r\n",
        "                    loss = loss_function(updated_q_values[i], q_action)\r\n",
        "\r\n",
        "                grads = tape.gradient(loss, model.model.trainable_variables)\r\n",
        "                optimizer.apply_gradients(zip(grads, model.model.trainable_variables))\r\n",
        "\r\n",
        "        if frame_count % update_target_network == 0:\r\n",
        "            model_target.model.set_weights(model.model.get_weights())\r\n",
        "            template = \"episode {}, frame count {}\"\r\n",
        "            print(template.format(episode_count, frame_count))\r\n",
        "            \r\n",
        "        env.episode_reward_history.append(episode_reward)\r\n",
        "        if env.done:\r\n",
        "            break\r\n",
        "\r\n",
        "    episode_count += 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0, frame count 100\n",
            "episode 1, frame count 200\n",
            "episode 1, frame count 300\n",
            "episode 2, frame count 400\n",
            "episode 2, frame count 500\n",
            "episode 3, frame count 600\n",
            "episode 3, frame count 700\n",
            "episode 4, frame count 800\n",
            "episode 4, frame count 900\n",
            "episode 5, frame count 1000\n",
            "episode 5, frame count 1100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-58dbece1ef30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon_random_frames\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-5-5b0b5f4f6349>\u001b[0m in \u001b[0;36mexplore\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_legal_moves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "id": "c00CEiV9BrlE",
        "outputId": "a4c3b074-ab9f-4a8c-ee32-30ac23c7203d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "czNrPw1BBrlL"
      }
    }
  ]
}