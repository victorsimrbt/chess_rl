{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from keras.layers import Dense,Flatten,Reshape\r\n",
    "from keras.layers.convolutional import Conv2D\r\n",
    "\r\n",
    "gamma = 0.99  \r\n",
    "epsilon = 1  \r\n",
    "epsilon_min = 0.1  \r\n",
    "epsilon_max = 1.0  \r\n",
    "epsilon_interval = (\r\n",
    "    epsilon_max - epsilon_min\r\n",
    ")  \r\n",
    "batch_size = 32  \r\n",
    "max_steps_per_episode = 200\r\n",
    "num_actions = 4096"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from IPython.display import clear_output\r\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\r\n",
    "\r\n",
    "running_reward = 0\r\n",
    "episode_count = 0\r\n",
    "frame_count = 0\r\n",
    "\r\n",
    "epsilon_random_frames = 50000\r\n",
    "epsilon_greedy_frames = 1000000.0\r\n",
    "max_memory_length = 100000\r\n",
    "update_after_actions = 4\r\n",
    "update_target_network = 100\r\n",
    "loss_function = keras.losses.Huber()\r\n",
    "len_episodes = 0\r\n",
    "iterations = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import chess\r\n",
    "from board_conversion import *\r\n",
    "class ChessEnv():\r\n",
    "    def __init__(self):\r\n",
    "        self.board = chess.Board()\r\n",
    "        self.action_history = []\r\n",
    "        self.state_history = []\r\n",
    "        self.state_next_history = []\r\n",
    "        self.rewards_history = []\r\n",
    "        self.done_history = []\r\n",
    "        self.episode_reward_history = []\r\n",
    "        pass\r\n",
    "    def translate_board(self):\r\n",
    "        return translate_board(self.board)\r\n",
    "    def reset(self):\r\n",
    "        self.board = chess.Board()\r\n",
    "        if len(self.rewards_history) > max_memory_length:\r\n",
    "            del self.rewards_history[:1]\r\n",
    "            del self.state_history[:1]\r\n",
    "            del self.state_next_history[:1]\r\n",
    "            del self.action_history[:1]\r\n",
    "            del self.done_history[:1]\r\n",
    "        return translate_board(self.board)\r\n",
    "    \r\n",
    "    def step(self,action):\r\n",
    "        reward = 0\r\n",
    "        done = False\r\n",
    "        \r\n",
    "        state = self.translate_board()\r\n",
    "        self.board.push(action)\r\n",
    "        \r\n",
    "        state_next = self.board\r\n",
    "        state_next = translate_board(state_next)\r\n",
    "        \r\n",
    "        if self.board.is_checkmate():\r\n",
    "            reward = 100\r\n",
    "        if self.board.is_game_over():\r\n",
    "            done = True\r\n",
    "\r\n",
    "        self.action_history.append(move2num[action])\r\n",
    "        self.state_history.append(state)\r\n",
    "        self.state_next_history.append(state_next)\r\n",
    "        self.done_history.append(done)\r\n",
    "        self.rewards_history.append(reward)\r\n",
    "        return state_next,reward,done\r\n",
    "        \r\n",
    "    def update_q_values(self):\r\n",
    "        indices = np.random.choice(range(len(self.done_history)), size=batch_size)\r\n",
    "            \r\n",
    "        state_sample = np.array([self.state_history[i] for i in indices])\r\n",
    "        state_next_sample = np.array([self.state_next_history[i] for i in indices])\r\n",
    "        rewards_sample = [self.rewards_history[i] for i in indices]\r\n",
    "        action_sample = [self.action_history[i] for i in indices]\r\n",
    "        done_sample = tf.convert_to_tensor(\r\n",
    "            [float(self.done_history[i]) for i in indices]\r\n",
    "        )\r\n",
    "        \r\n",
    "        future_rewards = model_target.model.predict(state_next_sample)\r\n",
    "        \r\n",
    "        updated_q_values = rewards_sample + gamma * tf.reduce_max(\r\n",
    "            future_rewards, axis=1\r\n",
    "        )\r\n",
    "\r\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample\r\n",
    "\r\n",
    "        masks = tf.one_hot(action_sample, num_actions)\r\n",
    "        return state_sample,masks,updated_q_values\r\n",
    "    \r\n",
    "env = ChessEnv()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "dictionary = {'white':[]}\r\n",
    "\r\n",
    "dictionary['white'].append(0)\r\n",
    "dictionary['white'].append(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Q_model():\r\n",
    "    def __init__(self):\r\n",
    "        self.model = self.create_q_model()\r\n",
    "\r\n",
    "    def create_q_model(self):\r\n",
    "    # Network defined by the Deepmind paper\r\n",
    "        input_layer = keras.Input(shape=(8, 8, 12))\r\n",
    "\r\n",
    "        # Convolutions on the frames on the screen\r\n",
    "        x = Conv2D(filters=64,kernel_size = 2,strides = (2,2))(input_layer)\r\n",
    "        x = Conv2D(filters=128,kernel_size=2,strides = (2,2))(x)\r\n",
    "        x = Conv2D(filters=256,kernel_size=2,strides = (2,2))(x)\r\n",
    "        x = Flatten()(x)\r\n",
    "\r\n",
    "        action = Dense(4096,activation = 'softmax')(x)\r\n",
    "        return keras.Model(inputs=input_layer, outputs=action)\r\n",
    "    \r\n",
    "    def predict(self,env):\r\n",
    "        state_tensor = tf.convert_to_tensor(env.translate_board())\r\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\r\n",
    "        action_probs = self.model(state_tensor, training=False)\r\n",
    "        action_space = filter_legal_moves(env.board,action_probs[0])\r\n",
    "        action = np.argmax(action_space, axis=None)\r\n",
    "        move= num2move[action]\r\n",
    "        return move,action\r\n",
    "    \r\n",
    "    def explore(self,env):\r\n",
    "        action_space = np.random.randn(4096)\r\n",
    "        action_space = filter_legal_moves(env.board,action_space)\r\n",
    "        action = np.argmax(action_space, axis=None)\r\n",
    "        move= num2move[action]\r\n",
    "        return move,action\r\n",
    "        \r\n",
    "    \r\n",
    "model = Q_model()\r\n",
    "model_target = Q_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for _ in range(iterations):\r\n",
    "    state = np.array(env.reset())\r\n",
    "    episode_reward = 0\r\n",
    "    len_episodes += 1\r\n",
    "    for timestep in range(1, max_steps_per_episode):\r\n",
    "        frame_count += 1\r\n",
    "\r\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\r\n",
    "            move,action = model.explore(env)\r\n",
    "        else:\r\n",
    "            move,action = model.predict(env)\r\n",
    "            \r\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\r\n",
    "        epsilon = max(epsilon, epsilon_min)\r\n",
    "        \r\n",
    "        state_next, reward, done = env.step(move)\r\n",
    "\r\n",
    "        episode_reward += reward\r\n",
    "\r\n",
    "        if frame_count % update_after_actions == 0 and len(env.done_history) > batch_size:\r\n",
    "            state_sample,masks,updated_q_values = env.update_q_values()\r\n",
    "            \r\n",
    "            with tf.GradientTape() as tape:\r\n",
    "                q_values = model.model(state_sample)\r\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\r\n",
    "                loss = loss_function(updated_q_values, q_action)\r\n",
    "\r\n",
    "            grads = tape.gradient(loss, model.model.trainable_variables)\r\n",
    "            optimizer.apply_gradients(zip(grads, model.model.trainable_variables))\r\n",
    "\r\n",
    "        if frame_count % update_target_network == 0:\r\n",
    "            model_target.model.set_weights(model.model.get_weights())\r\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\r\n",
    "            print(template.format(running_reward, episode_count, frame_count))\r\n",
    "            \r\n",
    "        env.episode_reward_history.append(episode_reward)\r\n",
    "        if done:\r\n",
    "            break\r\n",
    "\r\n",
    "    episode_count += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "running reward: 0.00 at episode 0, frame count 300\n",
      "running reward: 0.00 at episode 1, frame count 400\n",
      "running reward: 0.00 at episode 1, frame count 500\n",
      "running reward: 0.00 at episode 2, frame count 600\n",
      "running reward: 0.00 at episode 2, frame count 700\n",
      "running reward: 0.00 at episode 3, frame count 800\n",
      "running reward: 0.00 at episode 3, frame count 900\n",
      "running reward: 0.00 at episode 4, frame count 1000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8f77d74f9752>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_after_actions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone_history\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mstate_sample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mupdated_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-daedd9a0a2b8>\u001b[0m in \u001b[0;36mupdate_q_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mfuture_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 705\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2968\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2970\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2971\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   2972\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tf.version"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v2.version' from 'C:\\\\Users\\\\v_sim\\\\Miniconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "5d591f94785818b435df4881258bbd57da528693019fb2c63deaaf29b9986dd3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}